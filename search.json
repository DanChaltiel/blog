[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello, R!",
    "section": "",
    "text": "Competing risks\n\n\n\n\n\n\nR\n\n\ncode\n\n\nanalysis\n\n\nsurvival\n\n\n\nWhat model should we use when competing risks arise? \n\n\n\n\n\nJan 13, 2025\n\n\nDan Chaltiel\n\n\n\n\n\n\n\n\n\n\n\n\nShould I adjust on this one?\n\n\n\n\n\n\nR\n\n\ncode\n\n\nanalysis\n\n\nsimulation\n\n\nbias\n\n\n\nTo adjust or not to adjust, that is the question üíÄ \n\n\n\n\n\nJan 6, 2025\n\n\nDan Chaltiel\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\nHere we go! \n\n\n\n\n\nJan 4, 2025\n\n\nDan Chaltiel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-01_13_competing_risks/index.html",
    "href": "posts/2025-01_13_competing_risks/index.html",
    "title": "Competing risks",
    "section": "",
    "text": "TL;DR\n\n\n\nFine and Gray subdistribution model is designed for prediction and should not be used for causal analysis as it yields biased estimations of Hazard Ratios."
  },
  {
    "objectID": "posts/2025-01_13_competing_risks/index.html#introduction",
    "href": "posts/2025-01_13_competing_risks/index.html#introduction",
    "title": "Competing risks",
    "section": "Introduction",
    "text": "Introduction\nIf you are fitting a survival analysis on an endpoint that is not death, there are chances that you will face the problem of competing risks at some point.\nThe most known methods to address this problem are cause-specific censoring and Fine & Gray‚Äôs subdistribution method, although multistate models are entering the game.\nCompeting risks arise when an individual is at risk of experiencing multiple events of events, and the occurrence of one event precludes the occurrence of the others.\nFor example, in clinical trials, a patient may be at risk of cancer progression (the primary event of interest) but could also die from unrelated causes before progression occurs, making progression no longer observable.\nProperly accounting for competing risks is crucial, as standard survival methods like Kaplan-Meier can overestimate the probability of the event of interest by ignoring other risks that ‚Äúcompete‚Äù with it.\nHowever, the 2 methods are not equivalent, and we are about to show in what.\n\n\n\n\n\n\nDisclaimer\n\n\n\nA lot of the logic presented here comes from Paul Allison‚Äôs SAS code on the Statistical Horizons blog. I only wanted to reproduce his results in R and talk about multistate models."
  },
  {
    "objectID": "posts/2025-01_13_competing_risks/index.html#simulation",
    "href": "posts/2025-01_13_competing_risks/index.html#simulation",
    "title": "Competing risks",
    "section": "Simulation",
    "text": "Simulation\n\nData\nLet‚Äôs simulate some data to show the problem:\n\n\nShow the code for get_df()\nlibrary(tidyverse)\n\n#' Simulated dataset\n#' \n#' Simulate 2 variables `x` and `z` with normal distribution and `r` correlation,\n#' and 2 Weibull times depending on either `x` or `z`, then let them censor each\n#' other to mimic competing risks.\n#'\n#' @param n number of observations\n#' @param r correlation between `x` and `z`\n#' @param beta the true coefficient for `x` and `z`\n#' @param eos non-informative censoring (end of study)\n#' @param inf_censoring informative censoring (common risk factor for `x` & `z`)\n#' @param seed RNG seed\n#' \nget_df = function(n=10000, r=0.5, beta=0.5, eos=14, inf_censoring=FALSE, seed=NULL){\n  if(!is.null(seed)) set.seed(seed)\n  rtn = tibble(\n    id = seq(n),\n    # Generate a common risk factor if inf_censoring=TRUE\n    comrisk = inf_censoring * rnorm(n),\n    # Generate x and z, bivariate standard normal with r=.5\n    x = rnorm(n),\n    z = r*x + sqrt(1-r^2)*rnorm(n),\n    # Generate time_a (Weibull) depending on x with coefficient beta\n    logw = 2 + 1.5*(log(rexp(n)) - beta*x) + 2*comrisk,\n    time_a = exp(logw),\n    # Generate time_b (Weibull) depending on z with coefficient beta\n    logy = 2 + 1.5*(log(rexp(n)) - beta*z)+ 2*comrisk,\n    time_b = exp(logy),\n    # *Allow events to censor each other;\n    t = pmin(time_a, time_b, eos),\n    event_num = case_when(t&gt;=eos~0, time_b&gt;time_a~1, .default=2),\n    event = factor(event_num, 0:2, c(\"Censored\", \"Type A\", \"Type B\")),\n  )\n  rtn %&gt;% \n    select(id, t, event, x, z, time_a, time_b)\n}\n\n\n\ndf = get_df(n=10000, r=0.5, eos=8, inf_censoring=FALSE, seed=376)\ndf\n\n\n  \n\n\n\nHere, we have a dataset with, for each patient id, an observed time t and the event associated. The time t is either equal to the smaller time between time_a, time_b, and the administrative censoring time that we set to eos=8.\n\n\nRegression models\nOK, now let‚Äôs use this dataset to fit cause-specific Cox and Fine & Gray models, one of each for both Type A and Type B events:\n\n\nShow the code for get_models()\nlibrary(survival)\n\nget_models = function(df, variance=TRUE){\n  rtn = lst(\n    fg_a = cmprsk::crr(df$t, df$event, cbind(x=df$x, z=df$z), \n                       failcode=\"Type A\", cencode=\"Censored\", variance=variance),\n    fg_b = cmprsk::crr(df$t, df$event, cbind(x=df$x, z=df$z), \n                       failcode=\"Type B\", cencode=\"Censored\", variance=variance),\n    cox_a = coxph(Surv(t, event==\"Type A\") ~ x + z, data=df),\n    cox_b = coxph(Surv(t, event==\"Type B\") ~ x + z, data=df)\n  )\n  rtn\n}\n# cmprsk::crr can be long so we cache the result\nget_models = memoise::memoise(get_models, cache=cachem::cache_disk(\"cache\"))\nget_df = memoise::memoise(get_df, cache=cachem::cache_disk(\"cache\"))\n\n\n\nm = get_models(df)\nm %&gt;% \n  map(broom::tidy) %&gt;% \n  bind_rows(.id=\"model\") %&gt;% \n  transmute(model, term, estimate, p.value=format.pval(p.value, eps=0.01)) %&gt;% \n  separate(model, into=c(\"model\", \"event\")) %&gt;% \n  pivot_wider(names_from=model, values_from=c(estimate, p.value))\n\n\n  \n\n\n\nThe true effect is 0.5 for couples a/x and b/z, and 0 for couples a/z and b/x.\nWe can see that :\n\nCause-specific Cox model has very little bias in coefficient estimates, and p-values reflect the true data structure\nFine and Gray model underestimates the true effects and incorrectly assign a value for null coefficients.\n\n\n\nInformative censoring\nWe can do the same with informative censoring, i.e.¬†if type A and type B events share a common, unobserved risk factor.\n\ndf2 = get_df(n=10000, r=0.5, eos=8, inf_censoring=TRUE, seed=376)\n\nm2 = get_models(df2)\nm2 %&gt;% \n  map(broom::tidy) %&gt;% \n  bind_rows(.id=\"model\") %&gt;% \n  transmute(model, term, estimate, p.value=format.pval(p.value, eps=0.01)) %&gt;% \n  separate(model, into=c(\"model\", \"event\")) %&gt;% \n  pivot_wider(names_from=model, values_from=c(estimate, p.value))\n\n\n  \n\n\n\nInformative censoring is a known problem that is difficult to adress.\nAs you can see, both model are now biased. However, the cause-specific Cox model still gives less biased estimates than the Fine and Gray model."
  },
  {
    "objectID": "posts/2025-01_13_competing_risks/index.html#conclusion",
    "href": "posts/2025-01_13_competing_risks/index.html#conclusion",
    "title": "Competing risks",
    "section": "Conclusion",
    "text": "Conclusion\nFine and Gray subdistribution model is designed for prediction and should not be used for causal analysis as it yields biased estimations of Hazard Ratios. If you intend to interpret Hazard Ratios or their p-values, you should better use cause-specific Cox models."
  },
  {
    "objectID": "posts/2025-01_13_competing_risks/index.html#bonus-multistate-models",
    "href": "posts/2025-01_13_competing_risks/index.html#bonus-multistate-models",
    "title": "Competing risks",
    "section": "Bonus: Multistate models",
    "text": "Bonus: Multistate models\nAnother possibility is to use multistate models. For time-to-event data such as ours, the MultiState Hazard (MSH) model, implemented in package survival, is pretty much adapted as it is similar to the Cox model. See the ‚Äúcompete‚Äù vignette for more insight.\nFitting such a MSH model for competing risks is very straightforward: you use the coxph() function with event being a factor which first level should be the censoring.\nOur data is already constructed this way, so the code is:\n\nlibrary(survival)\nfit = coxph(Surv(t,event)~x+z, data=df, id=id)\nfit$transitions\n#&gt;         to\n#&gt; from     Type A Type B (censored)\n#&gt;   (s0)     4349   4260       1391\n#&gt;   Type A      0      0          0\n#&gt;   Type B      0      0          0\ntidy(fit) %&gt;% arrange(term)\n\n\n  \n\n\n\nAs expected, we get the exact same coefficients as with the 2 separate cause-specific models.\nThe interest about MSH models rises with plots, as the cause-specific Kaplan Meier estimator overestimates the risk in the presence of competing risks.\nTo illustrate this, lets binarise our x and y variables on whether they are &gt;0.\nFirst, you can see that mere binarisation itself introduces a large amount of bias, with x&gt;0 being wrongly associated with Type B events, and z&gt;0 with Type A events.\n\nbin_fit = coxph(Surv(t,event)~(x&gt;0)+(z&gt;0), data=df, id=id)\nbin_fit %&gt;% \n  tidy() %&gt;% \n  select(term, transition, estimate, p.value)\n\n\n  \n\n\n\nLet‚Äôs ignore that for a moment for the sake of the explanation. Here, we will only focus on the relationship between x and Type A events.\nLet‚Äôs plot both the Kaplan Meier risk curve of the cause-specific Cox model and the cumulative incidence of the MSH model:\n\nlibrary(ggsurvfit)\nsfit_bad = survfit(Surv(t, event==\"Type A\") ~ (x &gt; 0) + (z &gt; 0), data=df, id=id)\np_bad = ggsurvfit(sfit_bad[c(1,3)], type=\"risk\") +\n  add_confidence_interval() +\n  scale_ggsurvfit(y_scales=list(limits=c(0,1))) + \n  labs(title=\"Without competing risks\", y=\"Incidence of Type A events\")\n\nsfit_good = survfit(Surv(t, event) ~ (x &gt; 0) + (z &gt; 0),  data=df, id=id)\np_good = ggcuminc(sfit_good[c(1,3),], outcome=c(\"Type A\")) + \n  add_confidence_interval() +\n  scale_ggsurvfit(y_scales=list(limits=c(0,1))) + \n  labs(title=\"With competing risks\", y=\"Incidence of Type A events\")\n\npatchwork::wrap_plots(p_bad, p_good, guides=\"collect\") & theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\nAs you can see, while the cause-specific censoring approach is valid with Cox models, it overestimates the risk in Kaplan Meier curves."
  },
  {
    "objectID": "posts/2025-01_13_competing_risks/index.html#bonus-2-multiple-simulation",
    "href": "posts/2025-01_13_competing_risks/index.html#bonus-2-multiple-simulation",
    "title": "Competing risks",
    "section": "Bonus 2: Multiple simulation",
    "text": "Bonus 2: Multiple simulation\nThe first simulation is a nice proof of concept, but it is only one simulation on a very large sample.\nLet‚Äôs run it multiple times, so that we can see the changes on type I and type II errors:\n\n#this chunk runs in about 30 minutes\nN_runs = 1000\nN_sample = 200\ndata_list = seq(N_runs) %&gt;% \n  set_names(~paste0(\"seed\",.x)) %&gt;% \n  map(.progress=TRUE, ~{\n    d = get_df(n=N_sample, seed=.x)\n  })\n\nmodel_df = data_list %&gt;% \n  map(.progress=TRUE, ~{\n    get_models(.x) %&gt;% \n      map(~broom::tidy(.x, conf.int=TRUE))%&gt;% \n      bind_rows(.id=\"model\")\n  }) %&gt;%\n  bind_rows(.id=\"seed\") |&gt; \n  select(model, term, estimate, p.value, conf.low, conf.high) %&gt;%\n  separate(model, into=c(\"model\", \"event\")) %&gt;%\n  mutate(\n    real_estimate = ifelse(paste0(event, term) %in% c(\"ax\", \"bz\"), 0.5, 0),\n    .after=estimate\n  )\n#&gt;  ‚ñ†‚ñ†‚ñ†                                6% |  ETA: 22s\n#&gt;  ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                           21% |  ETA: 17s\n#&gt;  ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                      37% |  ETA: 13s\n#&gt;  ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                 53% |  ETA:  9s\n#&gt;  ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†             68% |  ETA:  6s\n#&gt;  ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†        83% |  ETA:  3s\n#&gt;  ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†    98% |  ETA:  0s\n\nmodel_df\n\n\n  \n\n\n\nNow, we can show that the type I error is kept to the prespecified alpha=5% for CS-Cox models while it rises to 55% for F&G models!\nThe difference is less important for the type II error: for a true coefficient of 0.5, we have a statistical power of 98% for CS-Cox and 92% for F&G models.\n\nmodel_df %&gt;% \n  summarise(\n    real_estimate = unique(real_estimate),\n    p_signif = mean(p.value&lt;0.05), \n    ci_coverage = mean(conf.low&lt;real_estimate & real_estimate&lt;conf.high),\n    .by = c(model, event, term)\n  ) %&gt;% \n  mutate_if(is.numeric, ~round(.x, 2)) |&gt; \n  arrange(real_estimate, model)\n\n\n  \n\n\n\nAnd here, we can see the variation of the coefficient estimations:\n\nmodel_df %&gt;% \n  ggplot() +\n  aes(y=estimate, x=event, fill=term) +\n  geom_hline(yintercept=c(0, 0.5), alpha=0.5) +\n  geom_boxplot() +\n  facet_wrap(~model) +\n  theme(legend.position=\"top\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Dan Chaltiel",
    "section": "",
    "text": "About me\n\nI‚Äôm just a programming nerd that ended up loving data analysis.\nI have a PharmD, and a PhD in Epidemiology, and I work as a biostatistician at the Gustave Roussy institute (CESP Team Oncostat)."
  },
  {
    "objectID": "posts/2025-01_06_adjustment/index.html#introduction",
    "href": "posts/2025-01_06_adjustment/index.html#introduction",
    "title": "Should I adjust on this one?",
    "section": "Introduction",
    "text": "Introduction\nConfusion bias is one of the most known and common bias that can affect our analyses. Less known is the collider bias, although its effect should not be underestimated.\nWhen dealing with confusion bias, the usual way is to adjust on confounding variables. However, adjusting on a collider is precisely what causes the collider bias.\n\n\n\n\n\n\nTip\n\n\n\nI highly recommend Harvard‚Äôs ‚ÄúDraw Your Assumptions Before Your Conclusions‚Äù free online course. It explains DAGs and uses them to illustrates all kinds of biases.\n\n\nIn this post, we will simulate data and illustrate how those biases happen, and how strong they can be depending on different parameters."
  },
  {
    "objectID": "posts/2025-01_06_adjustment/index.html#scenarios",
    "href": "posts/2025-01_06_adjustment/index.html#scenarios",
    "title": "Should I adjust on this one?",
    "section": "Scenarios",
    "text": "Scenarios\nLet‚Äôs take a simple case: you want to measure the association between Y (binary) and X (continuous), and your dataset contains another variable Z (continuous) correlated to X.\nWe will consider 3 causal scenarios, in which we are interested in the relationship between X and Y, with X being correlated with Z:\n\nScenario 1: Y is caused by X alone (regardless of Z), there is no bias.\nExample: Y is cancer, X is smoking, and Z is consumption of coffee.\n\n\n\n\n\n\nflowchart LR\n    X --&gt;|?| Y[Y=1]\n    Z &lt;--&gt; X\n    style X fill:#e8f2fc,stroke:#2986e9\n    style Y fill:#f4e3f3,stroke:#c160c1\n    style Z fill:#dff6f4,stroke:#44d4c4\n\n\n\n\n\n\n\nScenario 2: Y is caused by X and Z, there can be confounding bias and you probably should adjust on Z.\nExample: Y is cancer, X is smoking, and Z is non-healthy behaviour (e.g.¬†measured by diet quality).\n\n\n\n\n\n\nflowchart LR\n    X --&gt;|?| Y[Y=1]\n    Z &lt;--&gt; X\n    Z --&gt; Y\n    style X fill:#e8f2fc,stroke:#2986e9\n    style Y fill:#f4e3f3,stroke:#c160c1\n    style Z fill:#dff6f4,stroke:#44d4c4\n\n\n\n\n\n\n\nScenario 3: Y is caused by X, and Z is caused by X and Y, there is a collider bias if you adjust on Z.\nExample: Y is cancer, X is smoking, and Z is pulmonary infection.\n\n\n\n\n\n\nflowchart LR\n    X --&gt;|?| Y[Y=1]\n    Y --&gt; Z\n    X &lt;--&gt; Z\n    style X fill:#e8f2fc,stroke:#2986e9\n    style Y fill:#f4e3f3,stroke:#c160c1\n    style Z fill:#fff8e1,stroke:#ffc61b\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere, I used a double arrow to note a simple association. Z could cause X, X could cause Z, or an unknown factor W could cause X and Z. This is not usual in DAGs which should be acyclical."
  },
  {
    "objectID": "posts/2025-01_06_adjustment/index.html#simulation",
    "href": "posts/2025-01_06_adjustment/index.html#simulation",
    "title": "Should I adjust on this one?",
    "section": "Simulation",
    "text": "Simulation\nTo simulate this, we will need to:\n\ngenerate datasets with the appropriate correlation structure\napply a logistic regression on each and save the right attributes\ncross every possible scenario to have a good coverage of the problem\n\n\nData simulation function\nOK so first, we have to build a function that can simulate a dataset along these 3 scenarios.\n\n\n\n\n\n\nDisclaimer\n\n\n\nThe following code was inspired by the one published as supplemental material in the article of Paul H. Lee in Sci Rep, many thanks to the author.\n\n\nThis code generates:\n\nX as a normally distributed vector\nZ as a normally distributed vector with a fixed correlation to X.\nY as a binary vector that represents the outcome of a logistic regression influenced by X with coefficient beta_x (and Z with coefficient beta_z in the ‚Äúconfounding‚Äù scenario).\n\nThe only exception is in the ‚Äúcollider‚Äù scenario, where Z is generated (using this code) so it has a fixed correlation to both X and Y.\n\n\nExpand here for data_sim() code\nlibrary(tidyverse)\n\ninvlogit = function(x) 1/(1+exp(-x)) #inverse-logit function\n\ndata_sim = function(N, correlation, beta_x=0.2, beta_z=0.2, \n                    type=c(\"no_bias\", \"confounding\", \"collider\"),\n                    seed=NULL){\n  if(!is.null(seed)) set.seed(seed)\n  type = match.arg(type)\n  x = rnorm(N)\n  if(type==\"no_bias\"){\n    # Y is computed from X only: Z is an unrelated variable (noise)\n    z = correlation*x + sqrt(1-correlation^2) * rnorm(N)\n    y = rbinom(N, 1, 1-invlogit(-beta_x*x))\n  } else if(type==\"confounding\"){\n    # Y is computed from both X and Z: Z is a common cause (confounding bias)\n    z = correlation*x + sqrt(1-correlation^2) * rnorm(N)\n    y = rbinom(N, 1, 1-invlogit(-beta_x*x -beta_z*z))\n  } else if(type==\"collider\"){\n    # Z is computed from both X and Y: Z is a collider (collider bias)\n    y = rbinom(N, 1, 1-invlogit(-beta_x*x))\n    z = rcorrelated(x, y, correlation) #rcorrelated is a custom function\n  }\n  \n  tibble(x,z,y)\n}\n\nshow_cor = function(m) {m=round(cor(m), 3);m[\"z\",\"z\"]=NA;m[c(\"z\",\"y\"),c(\"x\",\"z\")]}\n\n\nThe correlations depend on the effect of X and Z on Y, but we can see that the fixed correlations are respected:\n\ndf = data_sim(1000, correlation=0.3, beta_x=9, beta_z=9, type=\"no_bias\", seed=42)\nshow_cor(df) #cor(X,Z)=0.3\n#&gt;       x     z\n#&gt; z 0.313    NA\n#&gt; y 0.776 0.236\ndf = data_sim(1000, correlation=0.3, beta_x=9, beta_z=9, type=\"confounding\", seed=42)\nshow_cor(df) #cor(X,Z)=0.3\n#&gt;       x     z\n#&gt; z 0.313    NA\n#&gt; y 0.634 0.639\ndf = data_sim(1000, correlation=0.3, beta_x=9, beta_z=9, type=\"collider\", seed=42)\nshow_cor(df) #cor(X,Z)=cor(Y,Z)=0.3\n#&gt;       x   z\n#&gt; z 0.300  NA\n#&gt; y 0.777 0.3\n\n\n\nLogistic regressions\nOK, we know how to make datasets, now let‚Äôs draw a bunch of them and make some regression so we can see the effect of adjusting on Z.\nIn the following function, we generate n_sim datasets and for each we fit:\n\none bare logistic model y~x\nand one adjusted logistic model y~x+z.\n\nThen we return the coefficient, standard error, and p-value of each variable of each model.\n\n\n\n\n\n\nTip\n\n\n\nAs running all these scenarios takes a significant amount of time, I‚Äôm using {memoise} to cache the result of each simulation.\n\n\n\n\nExpand here for simulate() code\npval = function(beta, se) 2-2*pnorm(abs(beta)/se) #helper to calculate the p-value\n\nsimulate = function(n_sim, sample_size, correlation, \n                    beta_x=0.2, beta_z=0.2, \n                    type=c(\"no_bias\", \"confounding\", \"collider\"),\n                    seed=NULL, verbose=TRUE){\n  if(!is.null(seed)) set.seed(seed)\n  if(verbose) {\n    message(glue::glue(\"- Simulating: n_sim={n_sim}, sample_size={sample_size}, correlation={correlation}, beta_x={beta_x}, beta_z={beta_z}, type={type}, seed={seed} \\n\\n\"))\n  }\n  \n  seq(n_sim) %&gt;% \n    map(~{\n      df = data_sim(N=sample_size, correlation=correlation, \n                    beta_x=beta_x, beta_z=beta_z, type=type, \n                    seed=NULL)\n      m_bare = glm(y~x,   family=binomial(link=\"logit\"), data=df)\n      m_adju = glm(y~x+z, family=binomial(link=\"logit\"), data=df)\n      \n      tibble(\n        simu  = .x,\n        seed = seed,\n        # data  = list(df), #makes the final object too heavy\n        sample_size = sample_size, correlation = correlation,\n        beta_x = beta_x, beta_z = beta_z, type = type, \n        m_bare_x_coef = coef(m_bare)[\"x\"],\n        m_bare_x_se = sqrt(vcov(m_bare)[\"x\",\"x\"]),\n        m_bare_x_p = pval(m_bare_x_coef, m_bare_x_se),\n        m_adju_x_coef = coef(m_adju)[\"x\"],\n        m_adju_x_se = sqrt(vcov(m_adju)[\"x\",\"x\"]),\n        m_adju_x_p = pval(m_adju_x_coef, m_adju_x_se),\n        m_adju_z_coef = coef(m_adju)[\"z\"],\n        m_adju_z_se = sqrt(vcov(m_adju)[\"z\",\"z\"]),\n        m_adju_z_p = pval(m_adju_z_coef, m_adju_z_se),\n      )\n    }, .progress=n_sim*sample_size&gt;1e5) %&gt;% \n    list_rbind()\n}\n\nsimulate = memoise::memoise(simulate, cache=cachem::cache_disk(\"cache/logistic_adj\"), \n                            omit_args=\"verbose\")\n\n\nJust to be sure, with 1000 reps of 200 observations with a correlation of 0, we get the correct coefficient for beta_x with a quite decent precision:\n\nsim1 = simulate(n_sim=1000, sample_size=200, beta_x=1, correlation=0,\n                type=\"no_bias\", seed=42)\nsim1 %&gt;% \n  summarise(n_sim=n(), m_bare_x_coef=mean(m_bare_x_coef))\n\n\n  \n\n\n\n\n\nApplication: gridsearch\nUsing expand.grid(), we can now design various scenarios, and then use simulate() on each.\nHere, I‚Äôm considering a lot of possibilities, so this will take a fair amount of time to compute the first time. Thanks to memoise though, the next times will take a few ms only!\n\n\nExpand here for sim_all code\n#generic scenarios\nscenarios1 = expand_grid(\n  sample_size = c(200, 500, 1000), \n  correlation = seq(0, 0.5, by=0.1),\n  beta_x = c(0.2),\n  beta_z = c(0.2),\n  type = c(\"no_bias\", \"confounding\", \"collider\")\n) \n\n#bias-exploring scenarios\nscenarios2 = expand_grid(\n  sample_size = c(500), \n  correlation = seq(0, 0.5, by=0.1),\n  beta_x = c(0, 0.2),\n  beta_z = seq(0, 0.5, by=0.1),\n  type = c(\"confounding\", \"collider\")\n) \n\nscenarios = bind_rows(scenarios1, scenarios2) %&gt;% distinct()\n\nset.seed(42)\nsim_all = \n  scenarios %&gt;% \n  mutate(scenario = row_number(), .before=1) %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    sim = {\n      message(\"Computing scenario \", scenario, \"/\", nrow(scenarios), \"\\n\", sep=\"\")\n      list(simulate(n_sim=1000, sample_size=sample_size, correlation=correlation, \n                    beta_x=beta_x, beta_z=beta_z,\n                    type=type, seed=NULL, verbose=TRUE))\n    }\n  ) %&gt;% \n  ungroup()\n\nsaveRDS(sim_all, \"logistic_adj.rds\")\n\n\nThere we are, one row per scenario:\n\nnrow(sim_all)\n#&gt; [1] 186\n\n\n\n\n\n\n\nTip\n\n\n\nI‚Äôm saving this in the file logistic_adj.rds, so you can use it directly if you want to play with these results."
  },
  {
    "objectID": "posts/2025-01_06_adjustment/index.html#results-coefficients",
    "href": "posts/2025-01_06_adjustment/index.html#results-coefficients",
    "title": "Should I adjust on this one?",
    "section": "Results: coefficients",
    "text": "Results: coefficients\nAt last, we can explore our results and make some plots!\n\nData\nFirst, we have to wrangle out simulations into a workable dataframe. This implies summarising each simulation (here I‚Äôm using mean, sd, median, and quantiles on the X coefficient), then applying a bit of dplyr and tidyr magic:\n\n\nExpand here for df_coef code\ndf_coef =\n  sim_all %&gt;% \n  rowwise() %&gt;% \n  mutate(sim = summarise(sim, \n                         across(c(m_bare_x_coef, m_adju_x_coef), \n                                lst(mean, sd, median, \n                                    pc25=~quantile(.x, 0.25), pc75=~quantile(.x, 0.75))))) %&gt;% \n  unpack(sim) %&gt;% \n  mutate(across(matches(\"m_...._._coef\"), \n                .fns=~(.x-beta_x)/beta_x, \n                .names=\"{.col}_error\")) %&gt;% \n  pivot_longer(matches(\"m.*_x_\"),\n               names_pattern=c(\"m_(.*)_x_(.*)\"), names_to=c(\"model\",\".value\"),\n               names_transform=\\(x) paste0(\"x_\",x)\n  ) %&gt;% \n  mutate(\n    type2 = factor(type, levels=c(\"no_bias\", \"confounding\", \"collider\"),\n                   labels=c(\"No bias\", \"Confounder\", \"Collider\")),\n    model2 = factor(model, levels=c(\"x_bare\", \"x_adju\"), \n                    labels=c(\"Bare model\", \"Adjusted model\")),\n    scenario = paste(type2, model2, sep=\" - \") %&gt;% fct_rev(),\n    effect_z = paste0(\"Coef(Z)=\",beta_z)\n  ) \n\ndatatable(df_coef, class='nowrap display')\n\n\n\n\n\n\n\n\nError on X depending on scenario\nNow, let‚Äôs plot the error on the coefficient for X, for each sample size (color) and scenario (column), and compare the bare model with the adjusted model (rows).\n\npd = position_dodge(width = .05)\ncapt = \"The dots and whiskers represent the median and the 25th and 75th percentiles of the X coefficient among the N simulated trial replicates.\" %&gt;% str_wrap(width=nchar(.)/2)\ndf_coef %&gt;% \n  filter(beta_x==0.2 & beta_z==0.2) %&gt;%\n  ggplot() +\n  aes(x=correlation, color=factor(sample_size), fill=factor(sample_size)) +\n  geom_pointrange(aes(y=x_coef_median_error, ymin=x_coef_pc25_error, ymax=x_coef_pc75_error), \n                  position=pd) +\n  geom_hline(yintercept=0, alpha=0.3) +\n  facet_wrap(model2~type2, scales=\"free_y\") +\n  scale_x_continuous(breaks=scales::breaks_width(0.1), labels=scales::label_percent()) +\n  scale_y_continuous(labels=scales::label_percent()) +\n  labs(x=\"Correlation between X and Z\", y=\"Relative error on X coefficient\", \n       color=\"Sample size\", fill=\"Sample size\",\n       caption=capt)\n\n\n\n\n\n\n\n\nAs you can see, the error is around 5% in most scenarios, except for 2:\n\nWhen there is a confounder that we don‚Äôt adjust on (overestimation in this case)\nWhen there is a collider that we adjust on (underestimation in this case)\n\nThe error is directly dependant on the correlation between X and Z, with a linear trend for the non-adjusted confounder and a non-linear trend for the adjusted collider.\nNote that we should probably not extrapolate on the functional form of this latter non-linear trend, as it is most likely dependant on the correlation structure in our simulation rather than on the type of bias. In our collider scenario, Z is correlated with both X and Y. In the original paper, the author used z &lt;- rnorm(sample_size) + cor_x_z*x + y, and it didn‚Äôt show this kind of pattern.\n\n\nError on X depending on effect of Z\nIf we fix coefficient beta_x to 0.2, we can now vary beta_x to see the difference in bias magnitude.\n\ndf_coef %&gt;% \n  filter(beta_x==0.2, sample_size==500) %&gt;%\n  filter(scenario==\"Confounder - Bare model\" | scenario==\"Collider - Adjusted model\") %&gt;% \n  ggplot() +\n  aes(x=correlation) +\n  geom_col(aes(y=x_coef_mean_error), \n           position=pd, alpha=0.1) +\n  geom_pointrange(aes(y=x_coef_median_error, ymin=x_coef_pc25_error, ymax=x_coef_pc75_error), \n                  position=pd) +\n  geom_hline(yintercept=0, alpha=0.3) +\n  facet_grid(scenario~effect_z, scales=\"free_y\") +\n  scale_x_continuous(breaks=scales::breaks_width(0.1), labels=scales::label_percent()) +\n  scale_y_continuous(labels=scales::label_percent()) +\n  labs(x=\"Correlation between X and Z\", y=\"Relative error on X coefficient\", \n       color=\"Sample size\", fill=\"Sample size\",\n       caption=capt)\n\n\n\n\n\n\n\n\nAs expected, in both cases, there is more bias when the confounder/collider Z has a greater effect on Y than X."
  },
  {
    "objectID": "posts/2025-01_06_adjustment/index.html#results-p-values",
    "href": "posts/2025-01_06_adjustment/index.html#results-p-values",
    "title": "Should I adjust on this one?",
    "section": "Results: p-values",
    "text": "Results: p-values\n\nData\nSame as before, we need to wrangle our data into a workable dataframe. Here, I‚Äôm using prop.test to get the confidence interval of the proportion of significant p-values at alpha=5%. Then I‚Äôm painfuly unnesting and unpacking into nice columns.\nWe set the hypothesis to H0 or H1 depending on whether beta_x==0 so that we can see the effect on risks alpha and beta.\n\n\nExpand here for df_pval code\ndf_pval =\n  sim_all %&gt;% \n  rowwise() %&gt;% \n  mutate(sim = summarise(sim, across(ends_with(\"_p\"), ~{\n    x = prop.test(sum(.x&lt;0.05), length(.x))\n    tibble(mean=x$estimate, mean_check=mean(.x&lt;0.05), inf=x$conf.int[1], sup=x$conf.int[2])\n  }))) %&gt;% \n  unnest(sim) %&gt;% \n  unpack(everything(), names_sep=\"_\") %&gt;% \n  pivot_longer(matches(\"_p_\"),\n               names_pattern=c(\"m_(.*)_(.)_p_(.*)\"), \n               names_to=c(\"model\",\"term\",\".value\"),\n               names_transform=\\(x) paste0(\"pval_\", x),\n  ) %&gt;% \n  mutate(\n    type2 = factor(type, levels=c(\"no_bias\", \"confounding\", \"collider\"),\n                   labels=c(\"No bias\", \"Confounder\", \"Collider\")),\n    model2 = factor(model, levels=c(\"pval_bare\", \"pval_adju\"),\n                    labels=c(\"Bare model\", \"Adjusted model\")),\n    scenario = paste(type2, model2, sep=\" - \") %&gt;% fct_rev(),\n    hypothesis = ifelse(beta_x==0,\"H0\", \"H1\"),\n    effect_z = paste0(\"Coef(Z)=\",beta_z)\n  )\n\n\n\n\nError on p-values-driven decisions: confusion bias\nAs we observed above, the confusion bias depends on the correlation between X and Z and on the strength of the effect of Z, Coef(Z).\nLet‚Äôs plot the proportion of significant values (alpha=5%) for X when Coef(X)=0 (H0) and when Coef(X)=0.2 (H1), and see how it depends on those variables.\n\ndf_pval %&gt;%\n  filter(term==\"pval_x\") %&gt;% #we don't really care about Z significance\n  filter(sample_size==500) %&gt;% #these sims were made on N=500 only\n  filter(scenario==\"Confounder - Bare model\") %&gt;%\n  ggplot(aes(x=correlation, y=pval_mean, ymin=pval_inf, ymax=pval_sup, color=factor(beta_z))) +\n  geom_pointrange(size=0.3) + geom_line() +\n  facet_wrap(~hypothesis) +\n  # facet_grid(hypothesis~beta_z) + #this plot is nice too\n  labs(x=\"Correlation between X and Z\", y=\"Proportion of p-value&lt;0.05 for X coefficient\", \n       color=\"Coef(Z)\") +\n  guides(color=guide_legend(nrow=1,byrow=TRUE))\n\n\n\n\n\n\n\n\nThe dots and whiskers represent the proportion of significant values over the 1000 simulations and its very small 95% confidence interval.\nUnder H0 (left panel), when Coef(Z)=0, i.e.¬†when there is no confusion bias, the proportion of significant values is stable at 5%. This is expected from the pre-specified alpha, and serves as internal validation. Then, the more you increase Coef(Z) and Cor(X,Z), the more false positives you get: the alpha risk increases.\nUnder H1 (right panel), when Coef(Z)=0, the proportion of significant values is stable at a value that represents the power of the test when there is no confusion bias. Then, the more you increase Coef(Z) and Cor(X,Z), the more true positives you get: the test is overpowered. Of course this is a far less problematic issue than the one above."
  },
  {
    "objectID": "posts/2025-01_01_welcome/index.html",
    "href": "posts/2025-01_01_welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I‚Äôve been wanting to blog about R for a long time now, so here we go!\nI‚Äôm not sure where I‚Äôm going yet, but I think this will be the home of my little tutorials or simulation studies. \n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{chaltiel2025,\n  author = {Chaltiel, Dan and Chaltiel, Dan},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2025-01-04},\n  url = {https://danchaltiel.github.io/blog/posts/2025-01_01_welcome/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nChaltiel, Dan, and Dan Chaltiel. 2025. ‚ÄúWelcome To My\nBlog.‚Äù January 4, 2025. https://danchaltiel.github.io/blog/posts/2025-01_01_welcome/."
  }
]